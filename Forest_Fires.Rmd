---
title: "DSC5103 Final Project: Forest Fires"
subtitle: 'Prediction of Burn Area using Meteorological Data'
author: "Section A1 Group 10"
date: "Oct 2018"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
    highlight: tango
    theme: yeti
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)  # set output width, turn off scientific notation for big numbers
# knitr::opts_chunk$set(echo = TRUE) # include=FALSE
```

# Introduction
We will be studying the Forest Fires Data Set from https://archive.ics.uci.edu/ml/datasets/forest+fires put together and studied by *Cortez, P., & Morais, A. D. J. R. (2007)* [1]. This dataset covers meteorological and spatio-temporal data for forest fires (between 2000 and 2003) in Portugal's Montesinho Natural Park, with 13 attributes each for 517 such incidents. Our target attribute from these 13 is 'area' - total burned area in hectares (ha). The corresponding weather data is that which is registered by sensors at the moment the fire was detected (or first broke out). We have: temp, RH (realtive humidity), wind (speed), rain (accumulated precipitation over the last 30 minutes). 

We also have DMC, DC, ISI and FFMC are components of the Canadian Forest Fire Weather Index (FWI) System [2], which measures the effects of fuel moisture and wind on fire behaviour. These have been calculated using consecutive daily observations of temperature, relative humidity, wind speed, and 24-hour rainfall - i.e. these are time-lagged and not instantaneous values, unlike the four weather variables. Roughly, the higher these components, the more the expected severity of the fire. 

Spatio-temporal data includes the month and day of the week that the incident occurred, and X and Y co-ordinates of the incident with respect to the park. As noted in [1], smaller fires are much more frequent. This is the case in this dataset, as well as with incidences of wildfires around the world, making this a difficult regression problem.

## Problem 

We work on the following regression problem:

* Predicting the burned area of a forest fire, given the weather conditions and FWI components at the time the fire breaks out.
    + In the future, this data can be monitored or obtained in real-time, and is non-costly. 
    + This prediction can provide instant feedback to the appropriate diaster response teams.
    + It could also be re-framed as a multi-class classification problem, by dividing the incidents based on severity - Small, Medium, Large - and predicting this, rather than the absoulute burned area.

## Motivation
The environmental damage, financial and infratstructure loss from forest fires (or wildfires) can be staggering. Reports from CoreLogic, a property and consumer information provider, estimate that the loss from the 2018 California Wildfires is between 15 and 19 billion USD [3]. With forest fires sweeping the globe and worsening forest fire seasons [4], there is need for a tool such as this to improve firefighting resource management and disaster response. For example, when there are simultaneous occurrences of wildfires, we would be able to prioritize and allocate resources appropriately: ground crew could respond to fires judged to be "smaller", with air support diverted to locations of larger fires.

## Evaluation Metrics
As this is a regression problem, our main evaluation metric is "test" (or validation) RMSE. We also consider MAD - Mean Absolute Devaince. The "test" set is 20% of the original data.

## Literature Review
The frame of reference for our regression task is work by *Cortez, P., & Morais, A. D. J. R. (2007)*. They compared performance of SVM, NN and multiple linear regression and RF from the 'rminer' package, with the best performance from SVM using just the four weather variables - rain, wind, temperature and humidity. While this dataset hasn't been studied extensively, similar work in this domain has been done by  *Özbayoglu, A. M., & Bozer, R. (2012)* [5]. They predicted burned area in hectares, as well as the size-class of the fire as big, medium or small, using SVM and multi-layer perceptrons, with data from 7,920 forest fires in Turkey between 2000 and 2009 (this dataset wasn't accessible to us).

# Exploratory Data Analysis
We did some initial EDA and visualization to get an idea of our next steps: the required data pre-processing and balancing.

## Schema
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
library(tidyverse)
library(psych)
library(caretEnsemble)
library(caret)
library(corrplot)
library(ModelMetrics)

# load data
data <- read.csv(file = "forestfires.csv", stringsAsFactors = T)
summary(data)
```


## Distribution of Target Class
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
ggplot(data,aes(data$area)) + geom_histogram()
summary(data$area)
```
We can see that the data is heavily skewed towards small forest fires. There are 247 entries with area = 0, for incidents where the resulting burned area was < 100 m^2. A logarithmic transofrmation of area would be useful in this case, and we can add one to the target column first since ln(0) approaches negative infinity.

## Some Visualizations
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# describe(data)

#columns
names(data)
#numeric data
names(Filter(is.numeric,data))
#factor data
names(Filter(is.factor,data))

#check NAs
sum(is.na(data))

# check correlation - there are some colinearity based on which we might drop features
corPlot(Filter(is.numeric,data))

# visualize relation between target and major predictors
pairs(dplyr::select(data,c('rain', 'wind', 'temp', 'RH', 'area', 'DC', 'ISI', 'FFMC')))

# day/month wise incidents
plot(data$day, col='purple')
plot(data$month, col='purple')
# there are more incidents on weekends (fri/sat/sun), it might mean that campers vactioning might have caused/spotted fires


#Day/Month wise area Burnt 
ggplot(data, aes(x=data$month,data$area)) + geom_boxplot(outlier.shape=NA) + 
  geom_jitter(col='red') + theme_bw()

ggplot(data, aes(x=data$day,data$area)) + geom_boxplot(outlier.shape=NA) + 
  geom_jitter(col='red') + theme_bw()
```

# Data Preparation


## Outlier Detection
We see from the box plot below that there are 63 outliers (value outside 1.5 times the inter-quartile range) that may skew our results. These are removed.
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
boxplot(data$area)
outlier_values <- boxplot.stats(data$area)$out
# We decided to remove burnt areas more than 200, by doing this we remove only 5 major outliers from the data set and keep rest.
data = dplyr::filter(data, data$area<200)
boxplot(data$area)
#removing months wiht 1 or 2 records to avoid prediction issues
data <- data[!(data$month=='jan' | data$month=='nov' | data$month=='may'),]
```

## Log Transformation
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
data$log_area <- log(data$area + 1)

# new distribution
summary(data$log_area)
ggplot(data,aes(data$log_area)) + geom_histogram()
```


## Feature Selection
The 'X' and 'Y' attributes are coordinates in the 9x9 grid representing the area of Portugal's Montesinho Park. Specific to this situation - doesn't make sense to use for a generalized tool, and are anyway not found to be crucial in predicting burned area.
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
data0 <- data # old data with all features

data <- dplyr::select(data, -X, -Y)
summary(data)
```

## Normalization
For SVM and k-NN, we use normalized data on the 0-1 scale. Area is also 0-1 normalized in this case, instead of log transformed. <insert explanation here>
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}

normalize_c = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

denormalize <- function(x, min_x, max_x) {
  return(x * (max_x - min_x) + min_x) 
}

# Xs - normalize all predictors + area, for SVM and KNN
data_norm <- dplyr::select(data, -log_area)
data_norm <- data_norm %>% mutate_at(names(Filter(is.numeric, data_norm)), normalize_c)

summary(data_norm)

# remove 'area' from data for other models so it isn't use as an additional predictor
area_ha <- data$area # original area
data <- dplyr::select(data, -area)

```

## Train-Validation Split
Next, we will prepare the training and test dataset.
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
#shuffle data 
data = data[sample(1:nrow(data)),]

# split training and test data 80/20
set.seed(17)

train.index <- createDataPartition(data$log_area, p = 0.8, list = FALSE)
test.index <- -train.index

# set.seed(1)
# N <- nrow(data)
# train.index <- sample(1:N, round(N*0.8))
# test.index <- - train.index
```

Separating the data for GBM use:
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}

library(xgboost)
# convert data for xgboost - onehot encoding is automatically done 
x.train <- model.matrix(log_area ~ ., data=data[train.index, ])[, -11]
y.train <- data[train.index, "log_area"]

dtrain <- xgb.DMatrix(data=x.train, label=y.train)

x.test <- model.matrix(log_area ~ ., data=data[test.index, ])[ ,-11]
y.test <- data[test.index, "log_area"]

```


# Feature Engineering/Creation
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}

```

# Modelling

## Elastic Net
```{r glmnet, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
library(glmnetUtils)
library(MASS)
library(boot)

model_lm = glm(log_area ~ .,
               data=data[train.index,], family = 'gaussian')
summary(model_lm)
glm.diag.plots(model_lm)


# inverse log - 1
test_pred_area_lm <- exp(predict(model_lm, data[test.index,])) - 1
rmse(test_pred_area_lm, area_ha[test.index]) # 13.27

##############
## STEP AIC ##
##############
model_step <- stepAIC(model_lm, direction = 'both')
model_step

test_pred_area_step <- exp(predict(model_step, newdata = data[test.index,])) - 1
rmse(test_pred_area_step, area_ha[test.index])

###########
## LASSO ##
###########

model_lasso = cv.glmnet(log_area~.,
               data=data[train.index,], nfolds=10, use.model.frame = T, alpha = 1)

lam_lasso = model_lasso$lambda.min

# TEST PREDICTIONS 
test_pred_area_lasso = exp(predict(model_lasso, s=lam_lasso, newdata = data[test.index,], exact=TRUE)) - 1
rmse(test_pred_area_lasso, area_ha[test.index]) # ~13


###########
## RIDGE ##
###########
model_ridge = cv.glmnet(log_area~.,
               data=data[train.index,], nfolds=10, use.model.frame = T, alpha = 0)

lam_ridge = model_ridge$lambda.min

# TEST PREDICTIONS 
test_pred_area_ridge = exp(predict(model_ridge, s=lam_ridge, data[test.index,], exact=TRUE)) - 1
rmse(test_pred_area_ridge, area_ha[test.index])  # ~13

```

## k-Nearest Neighbours Regression
```{r}
library(onehot)
library(class)
library(FNN)
library(Metrics)

#Encode factors to dummies for KNN
data_norm$month = as.factor(data_norm$month)
data_norm$day = as.factor(data_norm$day)

encoder = onehot(data_norm, max_levels = 100)
knn_data = as.data.frame(predict(encoder, data_norm))

#Separating train and test for KNN
train_knn = knn_data[train.index,]
test_knn = knn_data[-train.index,]

#Finding out optimal K
ks = 1:30
mse.train = numeric(length=length(ks))
mse.test  = numeric(length=length(ks))

for (i in seq(along=ks)) {
  model.train = knn.reg(train_knn, train_knn, train_knn$area, k=ks[i])
  model.test  = knn.reg(train_knn, test_knn, train_knn$area, k=ks[i])
  mse.train[i] = mean((train_knn$area - model.train$pred)^2)
  mse.test[i] = mean((test_knn$area - model.test$pred)^2)
}

k.opt = ks[which.min(mse.test)]

#KNN regression
knn_pred = knn.reg(train_knn, test_knn, train_knn$area, k=k.opt)
knn_rmse = rmse(test_knn$area, as.numeric(denormalize(knn_pred$pred,min(area_ha), max(area_ha))))
knn_rmse # 11.6
```


## Support Vector Machines
```{r}

library(e1071)

train_data = data_norm[train.index,]
test_data = data_norm[-train.index,]

#Linear kernel
svm_linear = svm(area ~ rain + wind + temp + RH, data=train_data, kernel = 'linear', cost = 1)
pred.linear = predict(svm_linear, test_data)
pred.linear.denormalized = as.numeric(denormalize(pred.linear,min(area_ha), max(area_ha)))
svm_lin_rmse = rmse(as.numeric(denormalize(test_data$area,min(area_ha),max(area_ha))), pred.linear.denormalized)
svm_lin_rmse # 7.05

#Tuning linear kernel
tune_linear = tune(svm, area ~ rain + wind + temp + RH,  data = train_data, kernel='linear',
                       ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:6), control = tune.control(sampling = "bootstrap", cross = 5, best.model = TRUE,fix = 2/3,nrepeat = 2)))
tuned_linear = tune_linear$best.model
tuned_linear_pred = predict(tuned_linear, test_data) 
tuned.pred.linear.denormalized = as.numeric(denormalize(tuned_linear_pred,min(area_ha), max(area_ha)))
svm_lin_tuned_rmse = rmse(as.numeric(denormalize(test_data$area,min(area_ha),max(area_ha))), tuned.pred.linear.denormalized)
svm_lin_tuned_rmse # 


#RBF kernel
svm_rbf = svm(area ~ rain + wind + temp + RH, data=train_data, kernel = 'radial', cost = 1)
pred.rbf = predict(svm_rbf, test_data)
pred.rbf.denormalized = as.numeric(denormalize(pred.rbf,min(area_ha), max(area_ha)))
svm_rbf_rmse = rmse(as.numeric(denormalize(test_data$area,min(area_ha),max(area_ha))), pred.rbf.denormalized)
svm_rbf_rmse # 7.6

#Tuning RBF kernel
tune_rbf = tune(svm, area ~ rain + wind + temp + RH,  data = train_data, kernel='radial',
                 ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:8), control = tune.control(sampling = "bootstrap", cross = 5, best.model = TRUE,fix = 2/3,nrepeat = 2)))
tuned_rbf = tune_rbf$best.model
tuned_rbf_pred = predict(tuned_rbf, test_data) 
tuned.pred.rbf.denormalized = as.numeric(denormalize(tuned_rbf_pred,min(area_ha), max(area_ha)))
svm_rbf_tuned_rmse = rmse(as.numeric(denormalize(test_data$area,min(area_ha),max(area_ha))), tuned.pred.rbf.denormalized)
svm_rbf_tuned_rmse #

#We also tried Polynomial kernel and Sigmoid kernel, but came to conclusion that Linear and Radial kernels outperform them with this data.
```


## Random Forest
```{r randomforest, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
library("randomForest")

# tune random forest (mtry) manually
mse.rfs <- rep(0, 10) # repeat '0' 10 times

for(m in 1:10){
    set.seed(12345)
    rf <- randomForest(log_area ~ ., data = data, subset = train.index, ntree=501, mtry=m)
    mse.rfs[m] <- rf$mse[501]
}

plot(1:10, mse.rfs, type = "b", xlab="mtry", ylab="OOB Error")

# fit a random forest model
set.seed(12345)
model.rf <- randomForest(log_area ~ ., data = data, subset=train.index, ntree=501, mtry=1)
plot(model.rf)

# predict
pred.rf <- predict(model.rf, newdata=data[test.index,]) 

# inverse log - 1
pred.rf <- exp(pred.rf) - 1

# rmse in test data
rmse(pred.rf, area_ha[test.index]) # 13.5
```

## XGBoost
```{r xgboost, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}

# fit a boosting model with optimal parameters - from running 9-xgboost_tuning.R with dtrain as input
# max_depth.opt <- 2
# eta.opt <- 0.005
# subsample.opt <- 1
# colsample.opt <- 1
# nrounds.opt <- 134
# 
max_depth.opt <- 1
eta.opt <- 0.01
subsample.opt <- 1
colsample.opt <- 0.6
nrounds.opt <- 287

set.seed(12345)
model.xgb <- xgboost(data=dtrain, objective="reg:linear", nrounds=nrounds.opt, max_depth = max_depth.opt, eta=eta.opt, subsample=subsample.opt, colsample_bytree=colsample.opt , verbose=0)

# predict
pred.xgb <- predict(model.xgb, x.test)

# inverse log
pred.xgb <- exp(pred.xgb) - 1

# rmse
rmse(pred.xgb, area_ha[test.index]) # ~13.415
```

### XGBoost Variable Importance Plot
```{r xgb varimp, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}

importance_matrix <- xgb.importance(model = model.xgb, feature_names = colnames(x.train))
importance_matrix
xgb.plot.importance(importance_matrix=importance_matrix)

```

## Ensemble 
```{r ensemble, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE, warning=FALSE}

area_corr <- cor(dplyr::select(data, -c("month", "day")))
corrplot(area_corr, method = "circle")

set.seed(17)
# folds <- createFolds(data_norm[train.index,"log_area"], 5)
data0 <- dplyr::select(data0, -log_area)
folds <- createFolds(data0[train.index,"area"], 5)

control <- trainControl(method='repeatedcv', number=5, repeats=2, index=folds, search = 'grid', savePredictions = 'final')

algos <- c('knn','svmLinear')

models <- caretList(area~., data=data0[train.index,], metric = 'RMSE', trControl=control, methodList=algos)

models_perf <- resamples(models)
modelCor(models_perf)

stack_control <- trainControl(method = 'repeatedcv', number = 2, repeats = 5)

stack_lm <- caretStack(models, method='lm',  trControl=stack_control)

pred_stack <- predict(stack_lm, data0[test.index,])

# # inverse log - 1
# pred_stack <- exp(pred_stack) - 1

## RMSE of Stacking Model
stack_accuracy <- rmse(pred_stack, area_ha[test.index])
stack_accuracy # 30

```

# Results and Model Comparison
* We select RMSE as our performance metric and observe that both __ give us pretty good results as compared to the other models. Hence, we stack the above two models to augment our results. We find that the combination of these also gives a similar positive result.
* We notice that not all variables are significant in predicting the area (dependent variable) from the correlation plot. Hence we drop all the index and RH and use temp instead which is highly correlated to all. We also drop the X and Y coordinates as these do not seem to affect the area.

```{r ensemble_varimp, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE, warning=FALSE}

## Stacking Model with VarImp

set.seed(17)

models_varimp <- caretList(log_area ~ temp + wind + rain, data=data[train.index,], metric = 'RMSE', trControl=control, methodList=algos)

models_perf_varimp <- resamples(models)
modelCor(models_perf_varimp)

stack_lm_varimp <- caretStack(models_varimp, method='lm',  trControl=stack_control)

pred_stack_varimp <- predict(stack_lm_varimp, data[test.index,])

# inverse log - 1
pred_stack_varimp <- exp(pred_stack_varimp) - 1

## Accuracy and RMSE of Stacking Model
stack_accuracy <- RMSE(pred_stack_varimp, area_ha[test.index])
stack_accuracy # 31.47

```


# Insights
* The final model using significant variables contains a similar RMSE as compared to using all independent variables. Hence, we can say that the area is indeed dependent on temperature, wind and rain. The other variables do not contribute in predicting the area. 
* The best and similar results obtained are by using SVM, KNN or a combination of both.


# Multi-Class Classification

## Data Preparation
```{r multiclass prep, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE, warning=FALSE}

# divide data into quartiles and label them


```

## Modelling
```{r multiclass model, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE, warning=FALSE}



```

## Discussion


# Conclusion
* It is difficult to predict the exact area which would be affected by a forest fire since our data consists of a huge range of area, and small fires tend to be more frequent.
* Due to the small size of the dataset and the skew towards small forest fires, the results vary noticeably with the train-validation split.
* We could probably segment the area into different classes of the Severity of Damage (Low, Medium, High and Fatal) and consider it as multi-class problem.

# References

1. Cortez, P., & Morais, A. D. J. R. (2007). A data mining approach to predict forest fires using meteorological data. Available at: http://www3.dsi.uminho.pt/pcortez/fires.pdf

2. Government of Canada. (2017, September 17). Canadian Wildland Fire Information System. Retrieved November 29, 2018 from http://cwfis.cfs.nrcan.gc.ca/background/summary/fwi

3. Insurance Journal. (2018, November 27). Report Puts Losses from California Wildfires at \$15B to $19B. Retrieved November 29, 2018, from https://www.insurancejournal.com/news/west/2018/11/27/510160.htm

4. Vidal, J. (2018, July 28). Fire, Fire Everywhere: The 2018 Global Wildfire Season Is Already Disastrous. Retrieved from https://www.huffingtonpost.in/entry/fire-fire-everywhere-the-2018-global-wildfire-season-is-already-disastrous_us_5b5a1271e4b0de86f494ed28

5. Özbayoglu, A. M., & Bozer, R. (2012). Estimation of the burned area in forest fires using computational intelligence techniques. Procedia Computer Science, 12, 282-287.

