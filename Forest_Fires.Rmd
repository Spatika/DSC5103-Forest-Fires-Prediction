---
title: "DSC5103 Final Project: Forest Fires"
subtitle: 'Prediction of Burn Area using Meteorological Data'
author: "Section A1 Group 10"
date: "Oct 2018"
output:
  html_document:
    highlight: tango
    theme: yeti
---
<!--
comments must be put in an HTML comment form
-->

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)  # set output width, turn off scientific notation for big numbers
# knitr::opts_chunk$set(echo = TRUE) # include=FALSE
```

## Introduction
In this project, we will be studying the Forest Fires Data Set(source/cite). This dataset covers meteorological data for forest fires in Portugal from 2008, with 13 attributes each for 517 data points. 

### Data Sources

1. https://archive.ics.uci.edu/ml/datasets/forest+fires

### Problem 

We will be studying the following problem(s):

* Predict the burned area of a forest fire, given the prevailing meteorological conditions.
    + This data can be obtained in real-time, and is non-costly, and this prediction can provide instant feedback to the appropriate diaster response teams.
    + Could be possibly be imbalanced problem, if converted to multi-class classification.


### Evaluation Metrics

Need to do some work on which one would be appropriate based on problem outlined above.

* Regression - RMSE, MAD
* Classification - recall, overall accuracy, AUC/ROC
* Bingo Classification Accuracy?
* 1-Away Classification Accuracy? 

## Motivation
With forest fires sweeping Southern California, leaving much destruction in their wake, there is need for a tool such as ours to improve firefighting resource management and disaster response. Spin some more stuff on why this work is important, and why we chose it/use case - what Xs you will get in the future to predict this Y.

## Literature Review
While topical, this data set hasn't been studied extensively. However, the frame of reference for our regression task is work by * P. Cortez and A. Morais (2007)*.

Kaggle/GitHub sources:

1. https://www.kaggle.com/elikplim/predict-the-burned-area-of-forest-fires
2. 

## Exploratory Data Analysis
We did some initial EDA and visualization to get an idea of our next steps: the required data pre-processing and balancing, and some hints on possible feature engineering. 

### Schema
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}

library(tidyverse)
library(formatR)
library(psych)

# load data
data <- read.csv(file = "forestfires.csv", stringsAsFactors = T)

summary(data)
str(data)
head(data)
```


### Distribution of Target Class
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
ggplot(data,aes(data$area)) + geom_histogram()

```

### EDA
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
describe(data)

names(data)
names(select_if(data, is.numeric))
names(select_if(data, is.factor))
head(Filter(is.numeric,data))
sum(is.na(data))

corPlot(Filter(is.numeric,data))

#Visualize realtion between target and major predictors
pairs(dplyr::select(data,c('rain', 'wind', 'temp', 'RH', 'area', 'DC', 'ISI', 'FFMC')))

#Day/Month wise incidents
plot(data$day, col='purple')
plot(data$month, col='purple')
#there are more incidents on weekends, it might mean that campers vactioning might have caused/spotted fires


#Day/Month wise area Burnt 
ggplot(data, aes(x=data$month,data$area)) + geom_boxplot(outlier.shape=NA) + 
  geom_jitter(col='red') + theme_bw()

ggplot(data, aes(x=data$day,data$area)) + geom_boxplot(outlier.shape=NA) + 
  geom_jitter(col='red') + theme_bw()
```

## Data Preparation

```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# clean the NA's
# heart <- na.omit(heart)
# # convert to factors
# heart$Sex <- as.factor(heart$Sex)
# heart$Fbs <- as.factor(heart$Fbs)
# heart$RestECG <- as.factor(heart$RestECG)
# heart$ExAng <- as.factor(heart$ExAng)
# heart$Slope <- as.factor(heart$Slope)
# heart$Ca <- as.factor(heart$Ca)
# summary(heart)
```

### Train-Validation Split
Next, we will prepare the training and test dataset, as well as the 10 CV folds, for later model comparison.
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# split training and test data 50/50
# N <- nrow(heart)
# set.seed(456)
# train.index <- sample(1:N, round(N/2))
# test.index <- - train.index
```

Let's separate the test data for GBM use.
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# x.test <- heart[test.index, 1:13]
# y.test <- heart[test.index, 14]
```


## Feature Engineering/Creation
```{r eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# x.test <- heart[test.index, 1:13]
# y.test <- heart[test.index, 14]
```

## Modelling

1. Elastic Net
```{r glmnet, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# x.test <- heart[test.index, 1:13]
# y.test <- heart[test.index, 14]
```

K-Nearest Neighbour regression
```{r}
library(onehot)
library(class)
library(caret)
library(FNN)
library(Metrics)

#encode factors for KNN
data$month = as.factor(data$month)
data$day = as.factor(data$day)

encoder = onehot(data, max_levels = 100)
knn_data = as.data.frame(predict(encoder, data))

#separate train and test for KNN
train_knn = knn_data[1:400,]
test_knn = knn_data[401:517,]

#Optimal K
ks = 1:30
mse.train = numeric(length=length(ks))
mse.test  = numeric(length=length(ks))

for (i in seq(along=ks)) {
  model.train = knn.reg(train_knn, train_knn, train_knn$X, k=ks[i])
  model.test  = knn.reg(train_knn, test_knn, train_knn$X, k=ks[i])
  mse.train[i] = mean((train_knn$X - model.train$pred)^2)
  mse.test[i] = mean((test_knn$X - model.test$pred)^2)
}

k.opt = ks[which.min(mse.test)]

#training KNN regression

knn_pred = knn.reg(train_knn, test_knn, train_knn$X, k=k.opt)
knn_rmse = rmse(test_knn$X, knn_pred$pred)
knn_rmse # 2.47

```


Support Vector Machines
```{r}
#### Support Vector Machines 

library(e1071)

train_data = data[1:400,]
test_data = data[401:517,]

#linear kernel
svm_linear = svm(X ~ ., data=train_data, kernel = 'linear', cost = 1)
pred.linear = predict(svm_linear, test_data)
svm_lin_rmse = rmse(test_data$X, pred.linear)
svm_lin_rmse # 2.4 

tune_linear = tune(svm, X ~ .,  data = train_data, kernel='linear',
                       ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:6)))
tuned_linear = tune_linear$best.model
tuned_linear_pred = predict(tuned_linear, test_data) 
svm_lin_tuned_rmse = rmse(test_data$X, tuned_linear_pred)
svm_lin_tuned_rmse # 2.16

#polynomial kernel
svm_poly = svm(X ~ ., data=train_data, kernel = 'polynomial', cost = 1)
pred.poly = predict(svm_poly, test_data)
svm_poly_rmse = rmse(test_data$X, pred.poly)
svm_poly_rmse # 61.6

tune_poly = tune(svm, X ~ .,  data = train_data, kernel='polynomial',
                   ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:6)))
tuned_poly = tune_poly$best.model
tuned_poly_pred = predict(tuned_poly, test_data) 
svm_poly_tuned_rmse = rmse(test_data$X, tuned_poly_pred)
svm_poly_tuned_rmse # 51.9

#RBF kernel
svm_rbf = svm(X ~ ., data=train_data, kernel = 'radial', cost = 1)
pred.rbf = predict(svm_rbf, test_data)
svm_rbf_rmse = rmse(test_data$X, pred.rbf)
svm_rbf_rmse # 2.12

tune_rbf = tune(svm, X ~ .,  data = train_data, kernel='radial',
                 ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:8)))
tuned_rbf = tune_rbf$best.model
tuned_rbf_pred = predict(tuned_rbf, test_data) 
svm_rbf_tuned_rmse = rmse(test_data$X, tuned_rbf_pred)
svm_rbf_tuned_rmse # 2.09

#Sigmoid kernel
svm_sigmoid = svm(X ~ ., data=train_data, kernel = 'sigmoid', cost = 1)
pred.sigmoid = predict(svm_sigmoid, test_data)
svm_sigmoid_rmse = rmse(test_data$X, pred.sigmoid)
svm_sigmoid_rmse # 2.22

tune_sigmoid = tune(svm, X ~ .,  data = train_data, kernel='sigmoid',
                ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:8)))
tuned_sigmoid = tune_sigmoid$best.model
tuned_sigmoid_pred = predict(tuned_sigmoid, test_data) 
svm_sigmoid_tuned_rmse = rmse(test_data$X, tuned_sigmoid_pred)
svm_sigmoid_tuned_rmse # 3.6

```


2. Random Forest
```{r randomforest, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# x.test <- heart[test.index, 1:13]
# y.test <- heart[test.index, 14]
```

3. XGBoost
```{r xgboost, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# x.test <- heart[test.index, 1:13]
# y.test <- heart[test.index, 14]
```

4. Ensemble 
```{r ensemble, eval=TRUE, echo=TRUE, message=FALSE, tidy=TRUE}
# x.test <- heart[test.index, 1:13]
# y.test <- heart[test.index, 14]
```

## Results and Model Comparison
* Relevant plots, tables, discussions

## Insights
* Variable importance plots, other interpretations.

## Conclusion
* Limitations, scope for improvement
* Multi-class classification by dividing burn area into buckets
* Outlier detection methods


## References
In proper APA/MLA format.

1. P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimaraes, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9. Available at:http://www3.dsi.uminho.pt/pcortez/fires.pdf

2.

